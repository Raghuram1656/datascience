{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all imports done\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf    \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# %matplotlib inline\n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "#from PIL import Image\n",
    "import PIL.Image as Image\n",
    "\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "\n",
    "import keras\n",
    "from keras import Sequential,layers\n",
    "from keras.layers  import Dense,Dropout,Conv2D,MaxPooling2D,Flatten,GlobalAveragePooling2D,Activation\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.activations import relu,softmax\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "#for one hot encoding\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import np_utils\n",
    "\n",
    "\n",
    "# fix random seed for reproducibility \n",
    "seed = 7 \n",
    "np.random.seed(seed)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Train images and labels processing\n",
    "\n",
    "# test images and labels processing\n",
    "\n",
    "\"\"\"There are some steps to be performed berfoe CNN\n",
    "\n",
    "Standardize the images like / = 255\n",
    "split into train and test validators\n",
    "build cnn model\n",
    "\n",
    "Input shape : \n",
    "4D tensor with shape: (batch, channels, rows, cols) if data_format is \"channels_first\" or \n",
    "4D tensor with shape: (batch, rows, cols, channels) if data_format is \"channels_last\".\n",
    "\n",
    "If you never set data_format, = \"channels_first\" or \"channels_last\" then by default it is \"channels_last\"\n",
    "\n",
    "\n",
    "Only the first layer will have input shape\n",
    "\n",
    "\n",
    "\n",
    "This is a \"multi-class softmax classification\"\n",
    "\"\"\"\n",
    "print(\"all imports done\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dir = 'C:/Users/susarlas/Desktop/poc/citx/train'\n",
    "train_images = []\n",
    "train_labels = []\n",
    "train_data = []\n",
    "\n",
    "\n",
    "\n",
    "test_dir  = 'C:/Users/susarlas/Desktop/poc/citx/test'\n",
    "test_images = []\n",
    "test_labels = []\n",
    "test_data = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "side = 48\n",
    "batch_size = 100\n",
    "\n",
    "img_size = 48\n",
    "num_channels = 3\n",
    "img_size_flat = img_size * img_size * num_channels\n",
    "img_shape = (img_size, img_size)\n",
    "fc_size=11 #size of the output of final FC layer\n",
    "\n",
    "img_width  = 48\n",
    "img_height = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train Test Image Labelling and Resizing\n",
    "\n",
    "\n",
    "# Train images and labels processing\n",
    "\n",
    "for file_name in os.listdir(train_dir):\n",
    "    img = Image.open(os.path.join(train_dir, file_name))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # resize the image read\n",
    "    img = img.resize((side,side), Image.ANTIALIAS) # resize images \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Reading as an Array to represent Neural Networks\n",
    "    \n",
    "    train_images.append(np.asarray(img))  # read image as np array into train_images\n",
    "#     train_images.append(img)  # read image as np array into train_images\n",
    "    \n",
    "    # Train Labels \n",
    "    \n",
    "    if file_name.endswith(\".jpg\"):\n",
    "        temp_label = file_name.split('.jpg')\n",
    "        temp_file_name = str(temp_label[0])\n",
    "        \n",
    "    if len(temp_file_name) > 0:\n",
    "            temp_label_2 = temp_file_name.split('_')\n",
    "            train_labels.append(temp_label_2[1])\n",
    "\n",
    "                \n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(train_labels) \n",
    "encoded_train_labels = encoder.transform(train_labels)\n",
    "train_labels = np_utils.to_categorical(encoded_train_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train images and labels processing\n",
    "for file_name in os.listdir(test_dir):\n",
    "    img = Image.open(os.path.join(test_dir, file_name))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # resize the image read\n",
    "    img = img.resize((side,side), Image.ANTIALIAS) # resize images \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Reading as an Array to represent Neural Networks\n",
    "    \n",
    "    test_images.append(np.asarray(img))  # read image as np array into train_images\n",
    "\n",
    "    \n",
    "    # test Labels \n",
    "    \n",
    "    if file_name.endswith(\".jpg\"):\n",
    "        temp_label = file_name.split('.jpg')\n",
    "        temp_file_name = str(temp_label[0])\n",
    "        \n",
    "    if len(temp_file_name) > 0:\n",
    "            temp_label_2 = temp_file_name.split('_')\n",
    "            test_labels.append(temp_label_2[1])\n",
    "\n",
    "                \n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(test_labels) \n",
    "encoded_test_labels = encoder.transform(test_labels)\n",
    "test_labels = np_utils.to_categorical(encoded_test_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of train image set 310\n",
      "X_data shape: (48, 48, 3)\n",
      "y_data shape: (310, 11)\n",
      "length of test image set 86\n",
      "X_data shape: (48, 48, 3)\n",
      "y_data shape: (86, 11)\n"
     ]
    }
   ],
   "source": [
    "# Train Test Splitting\n",
    "X_train = np.asarray(train_images) # i have already converted as array in the above lines of code\n",
    "y_train = np.array(train_labels)\n",
    "\n",
    "X_test  = np.asarray(test_images) # i have already converted as array in the above lines of code\n",
    "y_test  = np.array(test_labels)\n",
    "\n",
    "\n",
    "print('length of train image set',len(X_train))\n",
    "print('X_data shape:', X_train[0].shape)\n",
    "print('y_data shape:', y_train.shape)\n",
    "\n",
    "\n",
    "print('length of test image set',len(X_test))\n",
    "print('X_data shape:', X_test[0].shape)\n",
    "print('y_data shape:', y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inside\n",
      "done with layers\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "print('inside')\n",
    "# Initialising the CNN\n",
    "\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape1 = (3, img_width, img_height)\n",
    "else:\n",
    "    input_shape1 = (img_width, img_height, 3)\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# input\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), input_shape = input_shape1,  padding=\"same\",activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#layer1\n",
    "model.add(Conv2D(filters=32, kernel_size=(3,3),strides=(1, 1),activation='relu',padding=\"same\"))\n",
    "model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "#layer2\n",
    "model.add(Conv2D(filters=64, kernel_size=(3,3),strides=(1, 1),activation='relu',padding=\"same\"))\n",
    "model.add(MaxPooling2D(pool_size=(2,2),strides=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "# Flattening\n",
    "model.add(Flatten())\n",
    "\n",
    "\n",
    "#Dense Layer1\n",
    "model.add(Dense(832, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "\n",
    "\n",
    "# dense layer whose output size is fixed in the hyper parameter: fc_size=11\n",
    "# model.add(Dense(units = 11,activation = softmax(y_train,axis=-1)))\n",
    "\n",
    "model.add(Dense(units = 11,activation = (tf.nn.softmax)))\n",
    "\n",
    "\n",
    "# sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9)\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "optimizer_function = sgd\n",
    "\n",
    "# Compiling the CNN\n",
    "model.compile(optimizer = optimizer_function, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "print(\"done with layers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# #Standardization Process : Actually we standardize using the max dimension for each image. Here i used a different approach by taking the highest 255.\n",
    "\n",
    "# print(\"before standardization\")\n",
    "X_train = np.array((X_train/255.0),dtype=np.float32)\n",
    "X_test = np.array((X_test/255.0), dtype=np.float32)\n",
    "# print(\"after standardization\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before history\n",
      "Train on 310 samples, validate on 86 samples\n",
      "Epoch 1/250\n",
      "310/310 [==============================] - 15s 49ms/step - loss: 2.0643 - acc: 0.2484 - val_loss: 2.2465 - val_acc: 0.4419\n",
      "Epoch 2/250\n",
      "310/310 [==============================] - 12s 39ms/step - loss: 1.9766 - acc: 0.2548 - val_loss: 2.1217 - val_acc: 0.4419\n",
      "Epoch 3/250\n",
      "310/310 [==============================] - 12s 39ms/step - loss: 1.9703 - acc: 0.2806 - val_loss: 2.0824 - val_acc: 0.4419\n",
      "Epoch 4/250\n",
      "310/310 [==============================] - 12s 38ms/step - loss: 1.9570 - acc: 0.2581 - val_loss: 2.0859 - val_acc: 0.4419\n",
      "Epoch 5/250\n",
      "310/310 [==============================] - 12s 40ms/step - loss: 1.9365 - acc: 0.2645 - val_loss: 2.1059 - val_acc: 0.4419\n",
      "Epoch 6/250\n",
      "310/310 [==============================] - 12s 37ms/step - loss: 1.9423 - acc: 0.2677 - val_loss: 2.1905 - val_acc: 0.4419\n",
      "Epoch 7/250\n",
      "310/310 [==============================] - 11s 36ms/step - loss: 1.9304 - acc: 0.2806 - val_loss: 2.1863 - val_acc: 0.4419\n",
      "Epoch 8/250\n",
      "310/310 [==============================] - 11s 36ms/step - loss: 1.9393 - acc: 0.2710 - val_loss: 2.2257 - val_acc: 0.4419\n",
      "Epoch 9/250\n",
      "310/310 [==============================] - 12s 39ms/step - loss: 1.9143 - acc: 0.2581 - val_loss: 2.1671 - val_acc: 0.4419\n",
      "Epoch 10/250\n",
      "310/310 [==============================] - 12s 38ms/step - loss: 1.9242 - acc: 0.2806 - val_loss: 2.1653 - val_acc: 0.4419\n",
      "Epoch 11/250\n",
      "310/310 [==============================] - 11s 37ms/step - loss: 1.9249 - acc: 0.2613 - val_loss: 2.1582 - val_acc: 0.4419\n",
      "Epoch 12/250\n",
      "310/310 [==============================] - 11s 36ms/step - loss: 1.9325 - acc: 0.2645 - val_loss: 2.2033 - val_acc: 0.4419\n",
      "Epoch 13/250\n",
      "310/310 [==============================] - 12s 38ms/step - loss: 1.9316 - acc: 0.2871 - val_loss: 2.1352 - val_acc: 0.4419\n",
      "Epoch 14/250\n",
      "310/310 [==============================] - 12s 38ms/step - loss: 1.9105 - acc: 0.2871 - val_loss: 2.1123 - val_acc: 0.4419\n",
      "Epoch 15/250\n",
      "310/310 [==============================] - 12s 38ms/step - loss: 1.9254 - acc: 0.3097 - val_loss: 2.1291 - val_acc: 0.4419\n",
      "Epoch 16/250\n",
      "310/310 [==============================] - 12s 37ms/step - loss: 1.9073 - acc: 0.3065 - val_loss: 2.1215 - val_acc: 0.4419\n",
      "Epoch 17/250\n",
      "310/310 [==============================] - 12s 38ms/step - loss: 1.9203 - acc: 0.2806 - val_loss: 2.1036 - val_acc: 0.4419\n",
      "Epoch 18/250\n",
      "310/310 [==============================] - 11s 36ms/step - loss: 1.9154 - acc: 0.2548 - val_loss: 2.1870 - val_acc: 0.4419\n",
      "Epoch 19/250\n",
      "310/310 [==============================] - 11s 36ms/step - loss: 1.8988 - acc: 0.3032 - val_loss: 2.0927 - val_acc: 0.4419\n",
      "Epoch 20/250\n",
      "310/310 [==============================] - 11s 37ms/step - loss: 1.8943 - acc: 0.2968 - val_loss: 2.1104 - val_acc: 0.4419\n",
      "Epoch 21/250\n",
      "310/310 [==============================] - 11s 36ms/step - loss: 1.9064 - acc: 0.2903 - val_loss: 2.1924 - val_acc: 0.4419\n",
      "Epoch 22/250\n",
      "310/310 [==============================] - 11s 35ms/step - loss: 1.9084 - acc: 0.2645 - val_loss: 2.1551 - val_acc: 0.4419\n",
      "Epoch 23/250\n",
      "310/310 [==============================] - 11s 35ms/step - loss: 1.8769 - acc: 0.2903 - val_loss: 2.1663 - val_acc: 0.4419\n",
      "Epoch 24/250\n",
      "310/310 [==============================] - 12s 38ms/step - loss: 1.8831 - acc: 0.2903 - val_loss: 2.1632 - val_acc: 0.4419\n",
      "Epoch 25/250\n",
      "310/310 [==============================] - 11s 37ms/step - loss: 1.8763 - acc: 0.2903 - val_loss: 2.1726 - val_acc: 0.4419\n",
      "Epoch 26/250\n",
      "310/310 [==============================] - 11s 36ms/step - loss: 1.8925 - acc: 0.3194 - val_loss: 2.2244 - val_acc: 0.1163\n",
      "Epoch 27/250\n",
      "310/310 [==============================] - 12s 40ms/step - loss: 1.8668 - acc: 0.3129 - val_loss: 2.1287 - val_acc: 0.4419\n",
      "Epoch 28/250\n",
      "310/310 [==============================] - 12s 37ms/step - loss: 1.8527 - acc: 0.3097 - val_loss: 2.1396 - val_acc: 0.1279\n",
      "Epoch 29/250\n",
      "310/310 [==============================] - 12s 37ms/step - loss: 1.8518 - acc: 0.3323 - val_loss: 2.1285 - val_acc: 0.4651\n",
      "Epoch 30/250\n",
      "310/310 [==============================] - 11s 36ms/step - loss: 1.8252 - acc: 0.3290 - val_loss: 2.1063 - val_acc: 0.3953\n",
      "Epoch 31/250\n",
      "310/310 [==============================] - 11s 36ms/step - loss: 1.7967 - acc: 0.3581 - val_loss: 2.0474 - val_acc: 0.4651\n",
      "Epoch 32/250\n",
      "310/310 [==============================] - 11s 36ms/step - loss: 1.7979 - acc: 0.3258 - val_loss: 2.0876 - val_acc: 0.0930\n",
      "Epoch 33/250\n",
      "310/310 [==============================] - 12s 37ms/step - loss: 1.7292 - acc: 0.3968 - val_loss: 2.0671 - val_acc: 0.1512\n",
      "Epoch 34/250\n",
      "310/310 [==============================] - 11s 36ms/step - loss: 1.7162 - acc: 0.3710 - val_loss: 1.9881 - val_acc: 0.4651\n",
      "Epoch 35/250\n",
      "310/310 [==============================] - 11s 36ms/step - loss: 1.7119 - acc: 0.3871 - val_loss: 2.0055 - val_acc: 0.4535\n",
      "Epoch 36/250\n",
      "310/310 [==============================] - 11s 36ms/step - loss: 1.6296 - acc: 0.4161 - val_loss: 1.9271 - val_acc: 0.2791\n",
      "Epoch 37/250\n",
      "310/310 [==============================] - 12s 37ms/step - loss: 1.6116 - acc: 0.4194 - val_loss: 2.0132 - val_acc: 0.2442\n",
      "Epoch 38/250\n",
      "310/310 [==============================] - 11s 36ms/step - loss: 1.5792 - acc: 0.4452 - val_loss: 1.9834 - val_acc: 0.1628\n",
      "Epoch 39/250\n",
      "310/310 [==============================] - 12s 37ms/step - loss: 1.5066 - acc: 0.4677 - val_loss: 1.8225 - val_acc: 0.2791\n",
      "Epoch 40/250\n",
      "310/310 [==============================] - 12s 39ms/step - loss: 1.4852 - acc: 0.4581 - val_loss: 1.8068 - val_acc: 0.3953\n",
      "Epoch 41/250\n",
      "310/310 [==============================] - 12s 38ms/step - loss: 1.4610 - acc: 0.4742 - val_loss: 1.8622 - val_acc: 0.4070\n",
      "Epoch 42/250\n",
      "310/310 [==============================] - 12s 38ms/step - loss: 1.4443 - acc: 0.5065 - val_loss: 1.8038 - val_acc: 0.3140\n",
      "Epoch 43/250\n",
      "310/310 [==============================] - 12s 38ms/step - loss: 1.3909 - acc: 0.5065 - val_loss: 1.7492 - val_acc: 0.3488\n",
      "Epoch 44/250\n",
      "310/310 [==============================] - 11s 37ms/step - loss: 1.3453 - acc: 0.5226 - val_loss: 1.7194 - val_acc: 0.3488\n",
      "Epoch 45/250\n",
      "310/310 [==============================] - 12s 39ms/step - loss: 1.2572 - acc: 0.5903 - val_loss: 1.6535 - val_acc: 0.4302\n",
      "Epoch 46/250\n",
      "310/310 [==============================] - 11s 37ms/step - loss: 1.2440 - acc: 0.5710 - val_loss: 1.7619 - val_acc: 0.4419\n",
      "Epoch 47/250\n",
      "310/310 [==============================] - 11s 36ms/step - loss: 1.1796 - acc: 0.6194 - val_loss: 1.6000 - val_acc: 0.4535\n",
      "Epoch 48/250\n",
      "310/310 [==============================] - 15s 48ms/step - loss: 1.1534 - acc: 0.5903 - val_loss: 1.5791 - val_acc: 0.4651\n",
      "Epoch 49/250\n",
      "310/310 [==============================] - 14s 45ms/step - loss: 1.1217 - acc: 0.5968 - val_loss: 1.4503 - val_acc: 0.4302\n",
      "Epoch 50/250\n",
      "310/310 [==============================] - 12s 38ms/step - loss: 1.0928 - acc: 0.6258 - val_loss: 1.5001 - val_acc: 0.4651\n",
      "Epoch 51/250\n",
      "310/310 [==============================] - 12s 39ms/step - loss: 1.0335 - acc: 0.6419 - val_loss: 1.5834 - val_acc: 0.4070\n",
      "Epoch 52/250\n",
      "310/310 [==============================] - 11s 37ms/step - loss: 1.0670 - acc: 0.6194 - val_loss: 1.3881 - val_acc: 0.5233\n",
      "Epoch 53/250\n",
      "310/310 [==============================] - 12s 39ms/step - loss: 0.9898 - acc: 0.6742 - val_loss: 1.4692 - val_acc: 0.4651\n",
      "Epoch 54/250\n",
      "310/310 [==============================] - 11s 37ms/step - loss: 0.9266 - acc: 0.6742 - val_loss: 1.3649 - val_acc: 0.5698\n",
      "Epoch 55/250\n",
      "310/310 [==============================] - 11s 37ms/step - loss: 0.9289 - acc: 0.6903 - val_loss: 1.2642 - val_acc: 0.5581\n",
      "Epoch 56/250\n",
      "310/310 [==============================] - 11s 36ms/step - loss: 0.9149 - acc: 0.6839 - val_loss: 1.2963 - val_acc: 0.5698\n",
      "Epoch 57/250\n",
      "310/310 [==============================] - 12s 38ms/step - loss: 0.9264 - acc: 0.6806 - val_loss: 1.3135 - val_acc: 0.5349\n",
      "Epoch 58/250\n",
      "310/310 [==============================] - 12s 38ms/step - loss: 0.8468 - acc: 0.6871 - val_loss: 1.3603 - val_acc: 0.5698\n",
      "Epoch 59/250\n",
      "310/310 [==============================] - 12s 37ms/step - loss: 0.8248 - acc: 0.7387 - val_loss: 1.3095 - val_acc: 0.6047\n",
      "Epoch 60/250\n",
      "310/310 [==============================] - 11s 36ms/step - loss: 0.7636 - acc: 0.7194 - val_loss: 1.3104 - val_acc: 0.5698\n",
      "Epoch 61/250\n",
      "310/310 [==============================] - 11s 36ms/step - loss: 0.7938 - acc: 0.6903 - val_loss: 1.3352 - val_acc: 0.5698\n",
      "Epoch 62/250\n",
      "310/310 [==============================] - 11s 37ms/step - loss: 0.7554 - acc: 0.7387 - val_loss: 1.2194 - val_acc: 0.5930\n",
      "Epoch 63/250\n",
      "310/310 [==============================] - 12s 39ms/step - loss: 0.6818 - acc: 0.7516 - val_loss: 1.3166 - val_acc: 0.5814\n",
      "Epoch 64/250\n",
      "310/310 [==============================] - 11s 37ms/step - loss: 0.6824 - acc: 0.7677 - val_loss: 1.3433 - val_acc: 0.5814\n",
      "Epoch 65/250\n",
      "310/310 [==============================] - 11s 36ms/step - loss: 0.7379 - acc: 0.7452 - val_loss: 1.2752 - val_acc: 0.5814\n",
      "Epoch 66/250\n",
      "310/310 [==============================] - 11s 35ms/step - loss: 0.6321 - acc: 0.7774 - val_loss: 1.2004 - val_acc: 0.5814\n",
      "Epoch 67/250\n",
      "310/310 [==============================] - 12s 38ms/step - loss: 0.5867 - acc: 0.8129 - val_loss: 1.2168 - val_acc: 0.6163\n",
      "Epoch 68/250\n",
      "310/310 [==============================] - 11s 36ms/step - loss: 0.5985 - acc: 0.7903 - val_loss: 1.2990 - val_acc: 0.5930\n",
      "Epoch 69/250\n",
      "310/310 [==============================] - 12s 39ms/step - loss: 0.5878 - acc: 0.8065 - val_loss: 1.4262 - val_acc: 0.5465\n",
      "Epoch 70/250\n",
      "310/310 [==============================] - 12s 39ms/step - loss: 0.5615 - acc: 0.7935 - val_loss: 1.2056 - val_acc: 0.5698\n",
      "Epoch 71/250\n",
      "310/310 [==============================] - 11s 36ms/step - loss: 0.5006 - acc: 0.8065 - val_loss: 1.2124 - val_acc: 0.5814\n",
      "Epoch 72/250\n",
      "310/310 [==============================] - 11s 36ms/step - loss: 0.5039 - acc: 0.8323 - val_loss: 1.2121 - val_acc: 0.6395\n",
      "Epoch 73/250\n",
      "310/310 [==============================] - 11s 37ms/step - loss: 0.4311 - acc: 0.8613 - val_loss: 1.2933 - val_acc: 0.6395\n",
      "Epoch 74/250\n",
      "310/310 [==============================] - 11s 37ms/step - loss: 0.4242 - acc: 0.8419 - val_loss: 1.1538 - val_acc: 0.6512\n",
      "Epoch 75/250\n",
      "310/310 [==============================] - 12s 40ms/step - loss: 0.4549 - acc: 0.8387 - val_loss: 1.1429 - val_acc: 0.6977\n",
      "Epoch 76/250\n",
      "310/310 [==============================] - 12s 38ms/step - loss: 0.4380 - acc: 0.8484 - val_loss: 1.4729 - val_acc: 0.5581\n",
      "Epoch 77/250\n",
      "310/310 [==============================] - 11s 36ms/step - loss: 0.4125 - acc: 0.8355 - val_loss: 1.3163 - val_acc: 0.6047\n",
      "Epoch 78/250\n",
      "310/310 [==============================] - 11s 36ms/step - loss: 0.3845 - acc: 0.8613 - val_loss: 1.1778 - val_acc: 0.6512\n",
      "Epoch 79/250\n",
      "310/310 [==============================] - 13s 41ms/step - loss: 0.3782 - acc: 0.8742 - val_loss: 1.2665 - val_acc: 0.6395\n",
      "Epoch 80/250\n",
      "310/310 [==============================] - 12s 38ms/step - loss: 0.3128 - acc: 0.8839 - val_loss: 1.2895 - val_acc: 0.6628\n",
      "Epoch 81/250\n",
      "310/310 [==============================] - 12s 39ms/step - loss: 0.3225 - acc: 0.8871 - val_loss: 1.4827 - val_acc: 0.6395\n",
      "Epoch 82/250\n",
      "310/310 [==============================] - 12s 38ms/step - loss: 0.3034 - acc: 0.9065 - val_loss: 1.5769 - val_acc: 0.5814\n",
      "Epoch 83/250\n",
      "310/310 [==============================] - 11s 37ms/step - loss: 0.3009 - acc: 0.8903 - val_loss: 1.3059 - val_acc: 0.6395\n",
      "Epoch 84/250\n",
      "310/310 [==============================] - 11s 36ms/step - loss: 0.2914 - acc: 0.9000 - val_loss: 1.3644 - val_acc: 0.6395\n",
      "Epoch 85/250\n",
      "310/310 [==============================] - 11s 37ms/step - loss: 0.2512 - acc: 0.9129 - val_loss: 1.5112 - val_acc: 0.6395\n",
      "Epoch 86/250\n",
      "310/310 [==============================] - 12s 39ms/step - loss: 0.2587 - acc: 0.8903 - val_loss: 1.4970 - val_acc: 0.6279\n",
      "Epoch 87/250\n",
      "310/310 [==============================] - 12s 39ms/step - loss: 0.2755 - acc: 0.9000 - val_loss: 1.2389 - val_acc: 0.7093\n",
      "Epoch 88/250\n",
      "310/310 [==============================] - 12s 37ms/step - loss: 0.2192 - acc: 0.9355 - val_loss: 1.5080 - val_acc: 0.6279\n",
      "Epoch 89/250\n",
      "310/310 [==============================] - 11s 36ms/step - loss: 0.2002 - acc: 0.9355 - val_loss: 1.4881 - val_acc: 0.6744\n",
      "Epoch 90/250\n",
      "310/310 [==============================] - 11s 36ms/step - loss: 0.1597 - acc: 0.9387 - val_loss: 1.6249 - val_acc: 0.6279\n",
      "Epoch 91/250\n",
      "310/310 [==============================] - 12s 37ms/step - loss: 0.1786 - acc: 0.9419 - val_loss: 1.5235 - val_acc: 0.6860\n",
      "Epoch 92/250\n",
      "310/310 [==============================] - 12s 38ms/step - loss: 0.1938 - acc: 0.9258 - val_loss: 1.6568 - val_acc: 0.6395\n",
      "Epoch 93/250\n",
      "310/310 [==============================] - 12s 37ms/step - loss: 0.2052 - acc: 0.9419 - val_loss: 1.6930 - val_acc: 0.5814\n",
      "Epoch 94/250\n",
      "310/310 [==============================] - 11s 36ms/step - loss: 0.1820 - acc: 0.9323 - val_loss: 1.4928 - val_acc: 0.6628\n",
      "Epoch 95/250\n",
      "310/310 [==============================] - 11s 36ms/step - loss: 0.2048 - acc: 0.9290 - val_loss: 1.6277 - val_acc: 0.6512\n",
      "Epoch 96/250\n",
      "310/310 [==============================] - 12s 37ms/step - loss: 0.1581 - acc: 0.9484 - val_loss: 1.5681 - val_acc: 0.6395\n",
      "Epoch 97/250\n",
      "310/310 [==============================] - 11s 36ms/step - loss: 0.1489 - acc: 0.9548 - val_loss: 1.6770 - val_acc: 0.6628\n",
      "Epoch 98/250\n",
      "310/310 [==============================] - 12s 39ms/step - loss: 0.1326 - acc: 0.9548 - val_loss: 1.6876 - val_acc: 0.6628\n",
      "Epoch 99/250\n",
      "310/310 [==============================] - 12s 37ms/step - loss: 0.1780 - acc: 0.9419 - val_loss: 1.6368 - val_acc: 0.6628\n",
      "Epoch 100/250\n",
      "310/310 [==============================] - 11s 36ms/step - loss: 0.1162 - acc: 0.9677 - val_loss: 1.6696 - val_acc: 0.6628\n",
      "Epoch 101/250\n",
      "310/310 [==============================] - 11s 37ms/step - loss: 0.1636 - acc: 0.9419 - val_loss: 1.4711 - val_acc: 0.6977\n",
      "Epoch 102/250\n",
      "310/310 [==============================] - 11s 37ms/step - loss: 0.1556 - acc: 0.9452 - val_loss: 1.6931 - val_acc: 0.6628\n",
      "Epoch 103/250\n",
      "310/310 [==============================] - 12s 37ms/step - loss: 0.1410 - acc: 0.9484 - val_loss: 1.6662 - val_acc: 0.6628\n",
      "Epoch 104/250\n",
      "310/310 [==============================] - 12s 38ms/step - loss: 0.1322 - acc: 0.9677 - val_loss: 1.9292 - val_acc: 0.6163\n",
      "Epoch 105/250\n",
      "310/310 [==============================] - 11s 37ms/step - loss: 0.1464 - acc: 0.9452 - val_loss: 1.7251 - val_acc: 0.6279\n",
      "Epoch 106/250\n",
      "310/310 [==============================] - 11s 36ms/step - loss: 0.1151 - acc: 0.9613 - val_loss: 1.5878 - val_acc: 0.7093\n",
      "Epoch 107/250\n",
      "310/310 [==============================] - 12s 38ms/step - loss: 0.1299 - acc: 0.9452 - val_loss: 1.5880 - val_acc: 0.6744\n",
      "Epoch 108/250\n",
      "310/310 [==============================] - 11s 36ms/step - loss: 0.1231 - acc: 0.9548 - val_loss: 1.6866 - val_acc: 0.6860\n",
      "Epoch 109/250\n",
      "310/310 [==============================] - 12s 38ms/step - loss: 0.1187 - acc: 0.9645 - val_loss: 1.8539 - val_acc: 0.6047\n",
      "Epoch 110/250\n",
      "310/310 [==============================] - 12s 38ms/step - loss: 0.1217 - acc: 0.9548 - val_loss: 1.8312 - val_acc: 0.6628\n",
      "Epoch 111/250\n",
      "310/310 [==============================] - 12s 38ms/step - loss: 0.1121 - acc: 0.9581 - val_loss: 1.6881 - val_acc: 0.6744\n",
      "Epoch 112/250\n",
      "310/310 [==============================] - 12s 37ms/step - loss: 0.1106 - acc: 0.9548 - val_loss: 1.7515 - val_acc: 0.6512\n",
      "Epoch 113/250\n",
      "310/310 [==============================] - 12s 38ms/step - loss: 0.1300 - acc: 0.9548 - val_loss: 1.7416 - val_acc: 0.6279\n",
      "Epoch 114/250\n",
      "310/310 [==============================] - 12s 38ms/step - loss: 0.1146 - acc: 0.9677 - val_loss: 1.8320 - val_acc: 0.6628\n",
      "Epoch 115/250\n",
      "310/310 [==============================] - 12s 38ms/step - loss: 0.1009 - acc: 0.9645 - val_loss: 1.8358 - val_acc: 0.6860\n",
      "Epoch 116/250\n",
      "310/310 [==============================] - 12s 39ms/step - loss: 0.0792 - acc: 0.9710 - val_loss: 1.9034 - val_acc: 0.6744\n",
      "Epoch 117/250\n",
      "310/310 [==============================] - 12s 37ms/step - loss: 0.1076 - acc: 0.9613 - val_loss: 1.8546 - val_acc: 0.6628\n",
      "Epoch 118/250\n",
      "310/310 [==============================] - 12s 38ms/step - loss: 0.0893 - acc: 0.9677 - val_loss: 1.9600 - val_acc: 0.6047\n",
      "Epoch 119/250\n",
      "310/310 [==============================] - 11s 36ms/step - loss: 0.0907 - acc: 0.9645 - val_loss: 1.7356 - val_acc: 0.7209\n",
      "Epoch 120/250\n",
      "310/310 [==============================] - 12s 39ms/step - loss: 0.1018 - acc: 0.9742 - val_loss: 1.8188 - val_acc: 0.6628\n",
      "Epoch 121/250\n",
      "310/310 [==============================] - 12s 38ms/step - loss: 0.0854 - acc: 0.9677 - val_loss: 1.8335 - val_acc: 0.6628\n",
      "Epoch 122/250\n",
      "310/310 [==============================] - 12s 39ms/step - loss: 0.0719 - acc: 0.9806 - val_loss: 1.9022 - val_acc: 0.6512\n",
      "Epoch 123/250\n",
      "310/310 [==============================] - 11s 36ms/step - loss: 0.0625 - acc: 0.9839 - val_loss: 1.7645 - val_acc: 0.6860\n",
      "Epoch 124/250\n",
      "310/310 [==============================] - 12s 37ms/step - loss: 0.0611 - acc: 0.9839 - val_loss: 1.8499 - val_acc: 0.6163\n",
      "Epoch 125/250\n",
      "310/310 [==============================] - 12s 38ms/step - loss: 0.0709 - acc: 0.9871 - val_loss: 1.9923 - val_acc: 0.6744\n",
      "Epoch 126/250\n",
      "310/310 [==============================] - 11s 36ms/step - loss: 0.0610 - acc: 0.9774 - val_loss: 1.8802 - val_acc: 0.6860\n",
      "Epoch 127/250\n",
      "310/310 [==============================] - 12s 40ms/step - loss: 0.0600 - acc: 0.9871 - val_loss: 2.0655 - val_acc: 0.6395\n",
      "Epoch 128/250\n",
      "310/310 [==============================] - 13s 43ms/step - loss: 0.0713 - acc: 0.9774 - val_loss: 2.0366 - val_acc: 0.6395\n",
      "Epoch 129/250\n",
      "310/310 [==============================] - 11s 37ms/step - loss: 0.1040 - acc: 0.9548 - val_loss: 1.9007 - val_acc: 0.6860\n",
      "Epoch 130/250\n",
      "310/310 [==============================] - 12s 38ms/step - loss: 0.0749 - acc: 0.9710 - val_loss: 2.0366 - val_acc: 0.6860\n",
      "Epoch 131/250\n",
      "310/310 [==============================] - 12s 39ms/step - loss: 0.0730 - acc: 0.9806 - val_loss: 1.9788 - val_acc: 0.6395\n",
      "Epoch 132/250\n",
      "310/310 [==============================] - 12s 39ms/step - loss: 0.0649 - acc: 0.9871 - val_loss: 2.0437 - val_acc: 0.6628\n",
      "Epoch 133/250\n",
      "310/310 [==============================] - 11s 36ms/step - loss: 0.0619 - acc: 0.9871 - val_loss: 2.0427 - val_acc: 0.6279\n",
      "Epoch 134/250\n",
      "310/310 [==============================] - 12s 39ms/step - loss: 0.0420 - acc: 0.9871 - val_loss: 2.2037 - val_acc: 0.6395\n",
      "Epoch 135/250\n",
      "310/310 [==============================] - 11s 37ms/step - loss: 0.0481 - acc: 0.9839 - val_loss: 2.0423 - val_acc: 0.6628\n",
      "Epoch 136/250\n",
      "310/310 [==============================] - 12s 38ms/step - loss: 0.0577 - acc: 0.9839 - val_loss: 1.9687 - val_acc: 0.6628\n",
      "Epoch 137/250\n",
      "310/310 [==============================] - 12s 37ms/step - loss: 0.0603 - acc: 0.9774 - val_loss: 1.9095 - val_acc: 0.7093\n",
      "Epoch 138/250\n",
      "310/310 [==============================] - 12s 38ms/step - loss: 0.0803 - acc: 0.9710 - val_loss: 2.0874 - val_acc: 0.6512\n",
      "Epoch 139/250\n",
      "310/310 [==============================] - 11s 36ms/step - loss: 0.0470 - acc: 0.9839 - val_loss: 1.9568 - val_acc: 0.6860\n",
      "Epoch 140/250\n",
      "310/310 [==============================] - 12s 38ms/step - loss: 0.0746 - acc: 0.9806 - val_loss: 2.0697 - val_acc: 0.6744\n",
      "Epoch 141/250\n",
      "310/310 [==============================] - 12s 38ms/step - loss: 0.0584 - acc: 0.9806 - val_loss: 2.1099 - val_acc: 0.6512\n",
      "Epoch 142/250\n",
      "310/310 [==============================] - 12s 40ms/step - loss: 0.0392 - acc: 0.9903 - val_loss: 1.8623 - val_acc: 0.6977\n",
      "Epoch 143/250\n",
      "310/310 [==============================] - 12s 37ms/step - loss: 0.0495 - acc: 0.9806 - val_loss: 2.0685 - val_acc: 0.6395\n",
      "Epoch 144/250\n",
      "310/310 [==============================] - 11s 37ms/step - loss: 0.0425 - acc: 0.9968 - val_loss: 1.9905 - val_acc: 0.6977\n",
      "Epoch 145/250\n",
      "310/310 [==============================] - 12s 39ms/step - loss: 0.0602 - acc: 0.9710 - val_loss: 2.0943 - val_acc: 0.6512\n",
      "Epoch 146/250\n",
      "310/310 [==============================] - 12s 39ms/step - loss: 0.0365 - acc: 0.9935 - val_loss: 1.9509 - val_acc: 0.6628\n",
      "Epoch 147/250\n",
      "310/310 [==============================] - 12s 38ms/step - loss: 0.0524 - acc: 0.9806 - val_loss: 2.1434 - val_acc: 0.6395\n",
      "Epoch 148/250\n",
      "310/310 [==============================] - 12s 38ms/step - loss: 0.0299 - acc: 0.9935 - val_loss: 2.1736 - val_acc: 0.6628\n",
      "Epoch 149/250\n",
      "310/310 [==============================] - 12s 38ms/step - loss: 0.0407 - acc: 0.9871 - val_loss: 2.1580 - val_acc: 0.6395\n",
      "Epoch 150/250\n",
      "310/310 [==============================] - 11s 37ms/step - loss: 0.0323 - acc: 0.9935 - val_loss: 2.0196 - val_acc: 0.6628\n",
      "Epoch 151/250\n",
      "310/310 [==============================] - 12s 38ms/step - loss: 0.0299 - acc: 0.9935 - val_loss: 2.1858 - val_acc: 0.6512\n",
      "Epoch 152/250\n",
      "310/310 [==============================] - 12s 38ms/step - loss: 0.0348 - acc: 0.9871 - val_loss: 2.0981 - val_acc: 0.6744\n",
      "Epoch 153/250\n",
      "310/310 [==============================] - 12s 38ms/step - loss: 0.0372 - acc: 0.9839 - val_loss: 2.1288 - val_acc: 0.6860\n",
      "Epoch 154/250\n",
      "310/310 [==============================] - 12s 37ms/step - loss: 0.0447 - acc: 0.9774 - val_loss: 2.1304 - val_acc: 0.6512\n",
      "Epoch 155/250\n",
      "310/310 [==============================] - 11s 37ms/step - loss: 0.0438 - acc: 0.9871 - val_loss: 2.0316 - val_acc: 0.6744\n",
      "Epoch 156/250\n",
      "310/310 [==============================] - 12s 37ms/step - loss: 0.0398 - acc: 0.9871 - val_loss: 2.1431 - val_acc: 0.6628\n",
      "Epoch 157/250\n",
      "310/310 [==============================] - 12s 39ms/step - loss: 0.0374 - acc: 0.9903 - val_loss: 1.9861 - val_acc: 0.6860\n",
      "Epoch 158/250\n",
      "310/310 [==============================] - 12s 40ms/step - loss: 0.0398 - acc: 0.9871 - val_loss: 1.9073 - val_acc: 0.6860\n",
      "Epoch 159/250\n",
      "310/310 [==============================] - 11s 36ms/step - loss: 0.0305 - acc: 0.9935 - val_loss: 1.9679 - val_acc: 0.6977\n",
      "Epoch 160/250\n",
      "310/310 [==============================] - 12s 37ms/step - loss: 0.0157 - acc: 1.0000 - val_loss: 2.0857 - val_acc: 0.6744\n",
      "Epoch 161/250\n",
      "310/310 [==============================] - 11s 36ms/step - loss: 0.0425 - acc: 0.9871 - val_loss: 2.2144 - val_acc: 0.6744\n",
      "Epoch 162/250\n",
      "310/310 [==============================] - 12s 39ms/step - loss: 0.0220 - acc: 0.9968 - val_loss: 2.1150 - val_acc: 0.6860\n",
      "Epoch 163/250\n",
      "310/310 [==============================] - 12s 40ms/step - loss: 0.0277 - acc: 0.9871 - val_loss: 2.2575 - val_acc: 0.6628\n",
      "Epoch 164/250\n",
      "310/310 [==============================] - 12s 39ms/step - loss: 0.0493 - acc: 0.9871 - val_loss: 2.1237 - val_acc: 0.6628\n",
      "Epoch 165/250\n",
      "310/310 [==============================] - 13s 41ms/step - loss: 0.0302 - acc: 0.9871 - val_loss: 2.0835 - val_acc: 0.6744\n",
      "Epoch 166/250\n",
      "310/310 [==============================] - 12s 40ms/step - loss: 0.0327 - acc: 0.9935 - val_loss: 2.2479 - val_acc: 0.6744\n",
      "Epoch 167/250\n",
      "310/310 [==============================] - 12s 38ms/step - loss: 0.0399 - acc: 0.9903 - val_loss: 2.1251 - val_acc: 0.6628\n",
      "Epoch 168/250\n",
      "310/310 [==============================] - 12s 38ms/step - loss: 0.0166 - acc: 1.0000 - val_loss: 2.1809 - val_acc: 0.6744\n",
      "Epoch 169/250\n",
      "310/310 [==============================] - 13s 41ms/step - loss: 0.0316 - acc: 0.9903 - val_loss: 2.2075 - val_acc: 0.6628\n",
      "Epoch 170/250\n",
      "310/310 [==============================] - 12s 37ms/step - loss: 0.0476 - acc: 0.9871 - val_loss: 2.1854 - val_acc: 0.6628\n",
      "Epoch 171/250\n",
      "310/310 [==============================] - 12s 37ms/step - loss: 0.0287 - acc: 0.9935 - val_loss: 2.1214 - val_acc: 0.6744\n",
      "Epoch 172/250\n",
      "310/310 [==============================] - 12s 38ms/step - loss: 0.0183 - acc: 0.9935 - val_loss: 2.2822 - val_acc: 0.6628\n",
      "Epoch 173/250\n",
      "310/310 [==============================] - 12s 39ms/step - loss: 0.0607 - acc: 0.9871 - val_loss: 2.0612 - val_acc: 0.6744\n",
      "Epoch 174/250\n",
      "310/310 [==============================] - 13s 43ms/step - loss: 0.0328 - acc: 0.9903 - val_loss: 2.0565 - val_acc: 0.6744\n",
      "Epoch 175/250\n",
      "310/310 [==============================] - 12s 39ms/step - loss: 0.0164 - acc: 0.9968 - val_loss: 2.1083 - val_acc: 0.6977\n",
      "Epoch 176/250\n",
      "310/310 [==============================] - 12s 38ms/step - loss: 0.0148 - acc: 0.9968 - val_loss: 2.2533 - val_acc: 0.6744\n",
      "Epoch 177/250\n",
      "310/310 [==============================] - 12s 39ms/step - loss: 0.0389 - acc: 0.9871 - val_loss: 2.1966 - val_acc: 0.6744\n",
      "Epoch 178/250\n",
      "310/310 [==============================] - 12s 39ms/step - loss: 0.0454 - acc: 0.9871 - val_loss: 1.9466 - val_acc: 0.6512\n",
      "Epoch 179/250\n",
      "310/310 [==============================] - 12s 38ms/step - loss: 0.0272 - acc: 0.9935 - val_loss: 2.3471 - val_acc: 0.6512\n",
      "Epoch 180/250\n",
      "310/310 [==============================] - 13s 42ms/step - loss: 0.0408 - acc: 0.9903 - val_loss: 2.2317 - val_acc: 0.6628\n",
      "Epoch 181/250\n",
      "310/310 [==============================] - 12s 39ms/step - loss: 0.0235 - acc: 0.9968 - val_loss: 2.2495 - val_acc: 0.6512\n",
      "Epoch 182/250\n",
      "310/310 [==============================] - 12s 40ms/step - loss: 0.0370 - acc: 0.9871 - val_loss: 2.1649 - val_acc: 0.6744\n",
      "Epoch 183/250\n",
      "310/310 [==============================] - 11s 37ms/step - loss: 0.0271 - acc: 0.9935 - val_loss: 2.2746 - val_acc: 0.6628\n",
      "Epoch 184/250\n",
      "310/310 [==============================] - 12s 37ms/step - loss: 0.0413 - acc: 0.9871 - val_loss: 2.3233 - val_acc: 0.6395\n",
      "Epoch 185/250\n",
      "310/310 [==============================] - 12s 39ms/step - loss: 0.0330 - acc: 0.9903 - val_loss: 2.3097 - val_acc: 0.6512\n",
      "Epoch 186/250\n",
      "310/310 [==============================] - 12s 40ms/step - loss: 0.0295 - acc: 0.9935 - val_loss: 2.1791 - val_acc: 0.6512\n",
      "Epoch 187/250\n",
      "310/310 [==============================] - 12s 38ms/step - loss: 0.0509 - acc: 0.9806 - val_loss: 2.1818 - val_acc: 0.6744\n",
      "Epoch 188/250\n",
      "310/310 [==============================] - 12s 37ms/step - loss: 0.0357 - acc: 0.9839 - val_loss: 2.2621 - val_acc: 0.6512\n",
      "Epoch 189/250\n",
      "310/310 [==============================] - 12s 39ms/step - loss: 0.0382 - acc: 0.9871 - val_loss: 2.0885 - val_acc: 0.6628\n",
      "Epoch 190/250\n",
      "310/310 [==============================] - 12s 38ms/step - loss: 0.0315 - acc: 0.9903 - val_loss: 2.1947 - val_acc: 0.6512\n",
      "Epoch 191/250\n",
      "310/310 [==============================] - 12s 39ms/step - loss: 0.0359 - acc: 0.9935 - val_loss: 2.1774 - val_acc: 0.6744\n",
      "Epoch 192/250\n",
      "310/310 [==============================] - 13s 42ms/step - loss: 0.0207 - acc: 0.9935 - val_loss: 2.2026 - val_acc: 0.6512\n",
      "Epoch 193/250\n",
      "310/310 [==============================] - 12s 38ms/step - loss: 0.0309 - acc: 0.9903 - val_loss: 2.2762 - val_acc: 0.6744\n",
      "Epoch 194/250\n",
      "310/310 [==============================] - 12s 38ms/step - loss: 0.0153 - acc: 0.9935 - val_loss: 2.1715 - val_acc: 0.6628\n",
      "Epoch 195/250\n",
      "310/310 [==============================] - 12s 39ms/step - loss: 0.0139 - acc: 1.0000 - val_loss: 2.1552 - val_acc: 0.6744\n",
      "Epoch 196/250\n",
      "310/310 [==============================] - 12s 39ms/step - loss: 0.0168 - acc: 0.9968 - val_loss: 2.3579 - val_acc: 0.6628\n",
      "Epoch 197/250\n",
      "310/310 [==============================] - 12s 39ms/step - loss: 0.0141 - acc: 0.9968 - val_loss: 2.2551 - val_acc: 0.6744\n",
      "Epoch 198/250\n",
      "310/310 [==============================] - 12s 40ms/step - loss: 0.0314 - acc: 0.9935 - val_loss: 2.2331 - val_acc: 0.6512\n",
      "Epoch 199/250\n",
      "310/310 [==============================] - 12s 39ms/step - loss: 0.0427 - acc: 0.9839 - val_loss: 2.1954 - val_acc: 0.6860\n",
      "Epoch 200/250\n",
      "310/310 [==============================] - 12s 40ms/step - loss: 0.0103 - acc: 1.0000 - val_loss: 2.2925 - val_acc: 0.6628\n",
      "Epoch 201/250\n",
      "310/310 [==============================] - 13s 41ms/step - loss: 0.0355 - acc: 0.9839 - val_loss: 2.1990 - val_acc: 0.6860\n",
      "Epoch 202/250\n",
      "310/310 [==============================] - 12s 38ms/step - loss: 0.0282 - acc: 0.9968 - val_loss: 2.1960 - val_acc: 0.6628\n",
      "Epoch 203/250\n",
      "310/310 [==============================] - 12s 39ms/step - loss: 0.0330 - acc: 0.9806 - val_loss: 2.3003 - val_acc: 0.6744\n",
      "Epoch 204/250\n",
      "310/310 [==============================] - 12s 39ms/step - loss: 0.0256 - acc: 0.9935 - val_loss: 2.0410 - val_acc: 0.6860\n",
      "Epoch 205/250\n",
      "310/310 [==============================] - 12s 37ms/step - loss: 0.0187 - acc: 1.0000 - val_loss: 2.2712 - val_acc: 0.6860\n",
      "Epoch 206/250\n",
      "310/310 [==============================] - 12s 38ms/step - loss: 0.0209 - acc: 0.9935 - val_loss: 2.3890 - val_acc: 0.6744\n",
      "Epoch 207/250\n",
      "310/310 [==============================] - 12s 40ms/step - loss: 0.0238 - acc: 0.9935 - val_loss: 2.3309 - val_acc: 0.6744\n",
      "Epoch 208/250\n",
      "310/310 [==============================] - 12s 38ms/step - loss: 0.0085 - acc: 1.0000 - val_loss: 2.3655 - val_acc: 0.6860\n",
      "Epoch 209/250\n",
      "310/310 [==============================] - 12s 39ms/step - loss: 0.0171 - acc: 0.9935 - val_loss: 2.3671 - val_acc: 0.6744\n",
      "Epoch 210/250\n",
      "310/310 [==============================] - 12s 38ms/step - loss: 0.0362 - acc: 0.9806 - val_loss: 2.4774 - val_acc: 0.6512\n",
      "Epoch 211/250\n",
      "310/310 [==============================] - 12s 39ms/step - loss: 0.0154 - acc: 0.9968 - val_loss: 2.4136 - val_acc: 0.6628\n",
      "Epoch 212/250\n",
      "310/310 [==============================] - 12s 38ms/step - loss: 0.0437 - acc: 0.9839 - val_loss: 2.2842 - val_acc: 0.6628\n",
      "Epoch 213/250\n",
      "310/310 [==============================] - 12s 39ms/step - loss: 0.0365 - acc: 0.9968 - val_loss: 2.2171 - val_acc: 0.6744\n",
      "Epoch 214/250\n",
      "310/310 [==============================] - 12s 39ms/step - loss: 0.0118 - acc: 0.9968 - val_loss: 2.3910 - val_acc: 0.6628\n",
      "Epoch 215/250\n",
      "310/310 [==============================] - 12s 40ms/step - loss: 0.0124 - acc: 0.9935 - val_loss: 2.4729 - val_acc: 0.6512\n",
      "Epoch 216/250\n",
      "310/310 [==============================] - 12s 38ms/step - loss: 0.0270 - acc: 0.9871 - val_loss: 2.3092 - val_acc: 0.6628\n",
      "Epoch 217/250\n",
      "310/310 [==============================] - 13s 41ms/step - loss: 0.0142 - acc: 0.9968 - val_loss: 2.2163 - val_acc: 0.6860\n",
      "Epoch 218/250\n",
      "310/310 [==============================] - 12s 39ms/step - loss: 0.0096 - acc: 0.9968 - val_loss: 2.3864 - val_acc: 0.6628\n",
      "Epoch 219/250\n",
      "310/310 [==============================] - 12s 39ms/step - loss: 0.0090 - acc: 0.9968 - val_loss: 2.4315 - val_acc: 0.6628\n",
      "Epoch 220/250\n",
      "310/310 [==============================] - 12s 39ms/step - loss: 0.0169 - acc: 0.9935 - val_loss: 2.5032 - val_acc: 0.6279\n",
      "Epoch 221/250\n",
      "310/310 [==============================] - 12s 39ms/step - loss: 0.0241 - acc: 0.9871 - val_loss: 2.7976 - val_acc: 0.6395\n",
      "Epoch 222/250\n",
      "310/310 [==============================] - 12s 39ms/step - loss: 0.0174 - acc: 0.9935 - val_loss: 2.3967 - val_acc: 0.6860\n",
      "Epoch 223/250\n",
      "310/310 [==============================] - 12s 40ms/step - loss: 0.0104 - acc: 0.9968 - val_loss: 2.3843 - val_acc: 0.6860\n",
      "Epoch 224/250\n",
      "310/310 [==============================] - 12s 38ms/step - loss: 0.0220 - acc: 0.9903 - val_loss: 2.3328 - val_acc: 0.6628\n",
      "Epoch 225/250\n",
      "310/310 [==============================] - 12s 38ms/step - loss: 0.0317 - acc: 0.9903 - val_loss: 2.6413 - val_acc: 0.6512\n",
      "Epoch 226/250\n",
      "310/310 [==============================] - 12s 40ms/step - loss: 0.0322 - acc: 0.9806 - val_loss: 2.4684 - val_acc: 0.6628\n",
      "Epoch 227/250\n",
      "310/310 [==============================] - 13s 40ms/step - loss: 0.0208 - acc: 0.9968 - val_loss: 2.3509 - val_acc: 0.6512\n",
      "Epoch 228/250\n",
      "310/310 [==============================] - 13s 41ms/step - loss: 0.0109 - acc: 1.0000 - val_loss: 2.4095 - val_acc: 0.6860\n",
      "Epoch 229/250\n",
      "310/310 [==============================] - 12s 38ms/step - loss: 0.0189 - acc: 0.9968 - val_loss: 2.6343 - val_acc: 0.6395\n",
      "Epoch 230/250\n",
      "310/310 [==============================] - 12s 39ms/step - loss: 0.0246 - acc: 0.9871 - val_loss: 2.4169 - val_acc: 0.6512\n",
      "Epoch 231/250\n",
      "310/310 [==============================] - 13s 41ms/step - loss: 0.0092 - acc: 0.9968 - val_loss: 2.4413 - val_acc: 0.6744\n",
      "Epoch 232/250\n",
      "310/310 [==============================] - 12s 40ms/step - loss: 0.0077 - acc: 1.0000 - val_loss: 2.5962 - val_acc: 0.6512\n",
      "Epoch 233/250\n",
      "310/310 [==============================] - 12s 39ms/step - loss: 0.0466 - acc: 0.9871 - val_loss: 2.4129 - val_acc: 0.6860\n",
      "Epoch 234/250\n",
      "310/310 [==============================] - 12s 39ms/step - loss: 0.0223 - acc: 0.9903 - val_loss: 2.2260 - val_acc: 0.6977\n",
      "Epoch 235/250\n",
      "310/310 [==============================] - 12s 38ms/step - loss: 0.0159 - acc: 0.9968 - val_loss: 2.5224 - val_acc: 0.6628\n",
      "Epoch 236/250\n",
      "310/310 [==============================] - 12s 39ms/step - loss: 0.0123 - acc: 0.9968 - val_loss: 2.4709 - val_acc: 0.6395\n",
      "Epoch 237/250\n",
      "310/310 [==============================] - 12s 39ms/step - loss: 0.0125 - acc: 0.9935 - val_loss: 2.4347 - val_acc: 0.6628\n",
      "Epoch 238/250\n",
      "310/310 [==============================] - 12s 40ms/step - loss: 0.0256 - acc: 0.9903 - val_loss: 2.5225 - val_acc: 0.6163\n",
      "Epoch 239/250\n",
      "310/310 [==============================] - 12s 39ms/step - loss: 0.0118 - acc: 0.9968 - val_loss: 2.3087 - val_acc: 0.6860\n",
      "Epoch 240/250\n",
      "310/310 [==============================] - 12s 39ms/step - loss: 0.0117 - acc: 0.9968 - val_loss: 2.6226 - val_acc: 0.6744\n",
      "Epoch 241/250\n",
      "310/310 [==============================] - 12s 39ms/step - loss: 0.0295 - acc: 0.9903 - val_loss: 2.3364 - val_acc: 0.6744\n",
      "Epoch 242/250\n",
      "310/310 [==============================] - 12s 39ms/step - loss: 0.0085 - acc: 1.0000 - val_loss: 2.3528 - val_acc: 0.6860\n",
      "Epoch 243/250\n",
      "310/310 [==============================] - 13s 41ms/step - loss: 0.0319 - acc: 0.9871 - val_loss: 2.4397 - val_acc: 0.6744\n",
      "Epoch 244/250\n",
      "310/310 [==============================] - 12s 39ms/step - loss: 0.0065 - acc: 1.0000 - val_loss: 2.4603 - val_acc: 0.6744\n",
      "Epoch 245/250\n",
      "310/310 [==============================] - 12s 39ms/step - loss: 0.0100 - acc: 0.9968 - val_loss: 2.3123 - val_acc: 0.7093\n",
      "Epoch 246/250\n",
      "310/310 [==============================] - 12s 38ms/step - loss: 0.0130 - acc: 0.9968 - val_loss: 2.3537 - val_acc: 0.6860\n",
      "Epoch 247/250\n",
      "310/310 [==============================] - 12s 39ms/step - loss: 0.0252 - acc: 0.9871 - val_loss: 2.4338 - val_acc: 0.6860\n",
      "Epoch 248/250\n",
      "310/310 [==============================] - 12s 40ms/step - loss: 0.0093 - acc: 1.0000 - val_loss: 2.4467 - val_acc: 0.6860\n",
      "Epoch 249/250\n",
      "310/310 [==============================] - 13s 42ms/step - loss: 0.0083 - acc: 0.9968 - val_loss: 2.4628 - val_acc: 0.6860\n",
      "Epoch 250/250\n",
      "310/310 [==============================] - 12s 39ms/step - loss: 0.0320 - acc: 0.9903 - val_loss: 2.5114 - val_acc: 0.6860\n",
      "after history\n"
     ]
    }
   ],
   "source": [
    "# Train Model\n",
    "print(\"before history\")\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test,y_test),shuffle=True,epochs=250, batch_size=10,verbose=1)\n",
    "print('after history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# K.clear_session()\n",
    "# tf.reset_default_graph()\n",
    "# print(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 0s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "accuracy = model.evaluate(x=X_test,y=y_test,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 48, 48, 32)        896       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 48, 48, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 832)               7668544   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 832)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 11)                9163      \n",
      "=================================================================\n",
      "Total params: 7,706,347\n",
      "Trainable params: 7,706,347\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# save model and architecture to single file\n",
    "model.summary()\n",
    "from keras.models import load_model\n",
    "model.save('C:/Users/susarlas/Desktop/poc/citx/my_model_2.hd5')\n",
    "del model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session cleared\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "print(\"session cleared\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 48, 48, 32)        896       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 48, 48, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 832)               7668544   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 832)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 11)                9163      \n",
      "=================================================================\n",
      "Total params: 7,706,347\n",
      "Trainable params: 7,706,347\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "new_model = load_model('C:/Users/susarlas/Desktop/poc/citx/my_model_2.h5')\n",
    "print(new_model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 0, 1, 2, 3, 4, 6, 8, 9, 10, 0, 1, 3, 6, 8, 9, 0, 1, 6, 8, 9, 0, 1, 6, 8, 9, 0, 1, 6, 8, 9, 0, 1, 6, 8, 9, 0, 1, 6, 8, 9, 0, 1, 6, 8, 9, 0, 1, 6, 8, 9, 0, 1, 6, 8, 9, 0, 1, 6, 8, 9, 5, 0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 0, 1, 6, 8, 9, 0, 1, 6, 8, 9, 0, 1, 6, 8, 9, 0, 1, 6, 8, 9, 0, 1, 6, 8, 9, 0, 1, 6, 8, 9, 0, 6, 8, 9, 0, 6, 8, 9, 0, 6, 8, 9, 0, 6, 8, 9, 5, 0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 0, 6, 8, 9, 0, 6, 8, 9, 0, 6, 8, 9, 0, 6, 9, 0, 6, 9, 0, 6, 9, 0, 6, 9, 0, 6, 9, 0, 6, 9, 0, 6, 9, 5, 0, 1, 2, 3, 4, 6, 7, 8, 9, 0, 6, 9, 0, 6, 9, 0, 6, 9, 0, 6, 9, 0, 6, 9, 0, 6, 9, 0, 6, 9, 0, 6, 9, 0, 6, 9, 0, 6, 9, 0, 1, 2, 3, 6, 7, 8, 9, 0, 6, 9, 0, 6, 9, 0, 6, 9, 0, 6, 9, 0, 6, 9, 0, 6, 9, 0, 6, 9, 0, 9, 0, 9, 0, 9, 0, 1, 2, 3, 6, 7, 8, 9, 0, 9, 0, 9, 0, 9, 0, 9, 0, 9, 0, 0, 0, 0, 0, 0, 1, 2, 3, 6, 7, 8, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 6, 7, 8, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 6, 8, 9, 0, 0, 1, 3, 6, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# To Reverse Map Categorical values to the One Hot Encoded \n",
    "\n",
    "def one_hot_to_indices(data):\n",
    "    indices = []\n",
    "    for el in data:\n",
    "        indices.append(list(el).index(1))\n",
    "    return indices\n",
    "indices = one_hot_to_indices(train_labels)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
